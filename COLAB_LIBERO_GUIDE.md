# Google Colabì—ì„œ LIBERO Evaluation ì‹¤í–‰ ê°€ì´ë“œ
# Guide for Running LIBERO Evaluation on Google Colab

[í•œêµ­ì–´](#í•œêµ­ì–´-ê°€ì´ë“œ) | [English](#english-guide)

---

## í•œêµ­ì–´ ê°€ì´ë“œ

### ğŸ¯ ê°œìš”

ì´ ê°€ì´ë“œëŠ” Google Colabì—ì„œ OpenVLAì˜ LIBERO Simulation Benchmark Evaluationsë¥¼ ì‹¤í–‰í•  ë•Œ ë°œìƒí•˜ëŠ” ì¼ë°˜ì ì¸ ë¬¸ì œë“¤ì„ í•´ê²°í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.

### âš ï¸ ì£¼ìš” ë¬¸ì œì ë“¤

Colabì—ì„œ LIBERO evaluationì„ ì‹¤í–‰í•  ë•Œ ë‹¤ìŒê³¼ ê°™ì€ ë¬¸ì œë“¤ì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

1. **Flash Attention 2 í˜¸í™˜ì„± ë¬¸ì œ**
   - Colabì˜ GPU (íŠ¹íˆ T4)ì—ì„œ Flash Attention 2ê°€ ì§€ì›ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ
   - CUDA ë²„ì „ ë¶ˆì¼ì¹˜ë¡œ ì¸í•œ ì„¤ì¹˜ ì‹¤íŒ¨

2. **8ë¹„íŠ¸ ì–‘ìí™” ì˜¤ë¥˜**
   - `bitsandbytes` ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ transformers ë²„ì „ ì¶©ëŒ
   - CUDA ì»¤ë„ ë¡œë”© ì‹¤íŒ¨

3. **Dependency í˜¸í™˜ì„± ë¬¸ì œ**
   - PyTorch, transformers, tokenizers ë²„ì „ ë¶ˆì¼ì¹˜
   - Colabì˜ ê¸°ë³¸ ì„¤ì¹˜ íŒ¨í‚¤ì§€ì™€ ì¶©ëŒ

4. **ë©”ëª¨ë¦¬ ë¶€ì¡±**
   - T4 GPU (16GB)ì—ì„œ 7B ëª¨ë¸ ë¡œë”© ì‹œ ë©”ëª¨ë¦¬ ë¶€ì¡±
   - bfloat16ìœ¼ë¡œë„ ì•½ 14GB í•„ìš”

### ğŸš€ ë¹ ë¥¸ ì‹œì‘ (Colabì—ì„œ ì‹¤í–‰)

#### 1ë‹¨ê³„: ì €ì¥ì†Œ í´ë¡  ë° ì„¤ì •

```bash
# Colab ë…¸íŠ¸ë¶ì—ì„œ ì‹¤í–‰
!git clone https://github.com/openvla/openvla.git
%cd openvla

# Colab ì „ìš© ì„¤ì • ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
!python experiments/robot/libero/colab_setup_libero.py
```

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ìë™ìœ¼ë¡œ:
- GPU íƒ€ì… ê°ì§€ (T4, V100, A100 ë“±)
- ì ì ˆí•œ dependency ë²„ì „ ì„¤ì¹˜
- Flash Attention ì„¤ì¹˜ ì‹œë„ (ì‹¤íŒ¨í•´ë„ OK)
- LIBERO ë° í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜
- ë¬¸ì œ í•´ê²° íŒ ì œê³µ

#### 2ë‹¨ê³„: Evaluation ì‹¤í–‰

**A. V100/A100 GPUì˜ ê²½ìš° (ì–‘ìí™” ì—†ì´):**
```bash
!python experiments/robot/libero/run_libero_eval.py \
  --model_family openvla \
  --pretrained_checkpoint openvla/openvla-7b-finetuned-libero-spatial \
  --task_suite_name libero_spatial \
  --center_crop True
```

**B. T4 GPUì˜ ê²½ìš° (8ë¹„íŠ¸ ì–‘ìí™” ì‚¬ìš©):**
```bash
!python experiments/robot/libero/run_libero_eval.py \
  --model_family openvla \
  --pretrained_checkpoint openvla/openvla-7b-finetuned-libero-spatial \
  --task_suite_name libero_spatial \
  --center_crop True \
  --load_in_8bit True
```

### ğŸ”§ ë¬¸ì œ í•´ê²°

#### â­ ê°€ì¥ ì¤‘ìš”! í…ì„œ í¬ê¸° ë¶ˆì¼ì¹˜ ì˜¤ë¥˜ (291 vs 290)

```
Caught exception: The size of tensor a (291) must match the size of tensor b (290) at non-singleton dimension 3
```

**ì›ì¸**: Eager attention ëª¨ë“œì˜ OpenVLA êµ¬í˜„ì— í† í° ê¸¸ì´ ê³„ì‚° ë²„ê·¸ê°€ ìˆìŠµë‹ˆë‹¤. ì´ë¯¸ì§€ í† í°ê³¼ í…ìŠ¤íŠ¸ í† í°ì„ í•©ì¹  ë•Œ ë°œìƒí•©ë‹ˆë‹¤.

**âœ… í•´ê²°ì±…: SDPA ì‚¬ìš© (ê°•ë ¥ ì¶”ì²œ!)**

SDPA (Scaled Dot Product Attention)ëŠ”:
- âœ… PyTorch 2.0+ ë‚´ì¥ ê¸°ëŠ¥ (ì¶”ê°€ ì„¤ì¹˜ ë¶ˆí•„ìš”)
- âœ… T4 GPUì—ì„œ ì™„ë²½ í˜¸í™˜
- âœ… Flash Attentionì˜ 70-80% ì†ë„ (ì¶©ë¶„íˆ ë¹ ë¦„!)
- âœ… **í† í° ê¸¸ì´ ë²„ê·¸ ì—†ìŒ**
- âœ… 8ë¹„íŠ¸ ì–‘ìí™”ì™€ í•¨ê»˜ ì‚¬ìš© ê°€ëŠ¥

ì œê³µëœ `openvla_utils_colab.py`ëŠ” ìë™ìœ¼ë¡œ ë‹¤ìŒ ìˆœì„œë¡œ ì‹œë„í•©ë‹ˆë‹¤:
1. Flash Attention 2 (ê°€ì¥ ë¹ ë¦„)
2. **SDPA (T4ì—ì„œ ì¶”ì²œ!)** â­
3. Eager (ë§ˆì§€ë§‰ ìˆ˜ë‹¨, ë²„ê·¸ ìˆìŒ)

**ì‚¬ìš© ë°©ë²•**:
```bash
# Colab ìµœì í™” ë²„ì „ ì‚¬ìš© (ìë™ìœ¼ë¡œ SDPA ì‚¬ìš©)
!cp experiments/robot/openvla_utils_colab.py experiments/robot/openvla_utils.py

# ì‹¤í–‰ (SDPAê°€ ìë™ìœ¼ë¡œ ì„ íƒë¨)
!python experiments/robot/libero/run_libero_eval.py \
  --model_family openvla \
  --pretrained_checkpoint openvla/openvla-7b-finetuned-libero-spatial \
  --task_suite_name libero_spatial \
  --center_crop True \
  --load_in_8bit True
```

#### Flash Attention ì˜¤ë¥˜ê°€ ë°œìƒí•˜ëŠ” ê²½ìš°

```
ValueError: FlashAttention only support fp16 and bf16 data type
```

**í•´ê²°ì±…**: ìœ„ì˜ `openvla_utils_colab.py`ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”. ìë™ìœ¼ë¡œ SDPAë¡œ ì „í™˜í•©ë‹ˆë‹¤ (eagerê°€ ì•„ë‹Œ SDPA!)

#### 8ë¹„íŠ¸ ì–‘ìí™” ì˜¤ë¥˜

```
RuntimeError: CUDA error: no kernel image is available for execution on the device
```
ë˜ëŠ”
```
ImportError: bitsandbytes CUDA kernel loading failed
```

**ì›ì¸**: `bitsandbytes` ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ CUDA/transformers ë²„ì „ ë¶ˆì¼ì¹˜

**âœ… í•´ê²°ì±… 1 (ê¶Œì¥)**: í˜¸í™˜ë˜ëŠ” ë²„ì „ ì¬ì„¤ì¹˜
```bash
# bitsandbytes ìµœì‹  ë²„ì „ + transformers ì •í™•í•œ ë²„ì „
!pip uninstall -y bitsandbytes transformers
!pip install bitsandbytes>=0.43.0
!pip install transformers==4.40.1

# Colab ëŸ°íƒ€ì„ ì¬ì‹œì‘ í›„ ë‹¤ì‹œ ì‹¤í–‰
```

**âœ… í•´ê²°ì±… 2**: BitsAndBytesConfig ì‚¬ìš© (ìë™ ì²˜ë¦¬ë¨)
```python
# openvla_utils_colab.pyê°€ ìë™ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤
# ìˆ˜ë™ìœ¼ë¡œ ì„¤ì •í•  í•„ìš” ì—†ìŒ!
```

**í•´ê²°ì±… 3**: ì–‘ìí™” ì—†ì´ ì‹¤í–‰ (V100/A100ì—ì„œë§Œ)
```bash
# --load_in_8bit ì˜µì…˜ ì œê±°
# ì£¼ì˜: T4 GPUì—ì„œëŠ” ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ ì‹¤íŒ¨í•  ìˆ˜ ìˆìŒ
```

**í•´ê²°ì±… 4**: 4ë¹„íŠ¸ ì–‘ìí™” ì‹œë„ (ë” ì‘ì€ ë©”ëª¨ë¦¬)
```bash
--load_in_4bit True  # 8ë¹„íŠ¸ ëŒ€ì‹  4ë¹„íŠ¸
```

#### CUDA Out of Memory

```
torch.cuda.OutOfMemoryError: CUDA out of memory
```

**í•´ê²°ì±… 1**: 8ë¹„íŠ¸ ì–‘ìí™” ì‚¬ìš©
```bash
--load_in_8bit True
```

**í•´ê²°ì±… 2**: trials ìˆ˜ ì¤„ì´ê¸°
```bash
--num_trials_per_task 10  # ê¸°ë³¸ê°’ 50ì—ì„œ ì¤„ì„
```

**í•´ê²°ì±… 3**: ëŸ°íƒ€ì„ ì¬ì‹œì‘ ë° ë©”ëª¨ë¦¬ ì •ë¦¬
```python
import torch
torch.cuda.empty_cache()
```

#### Dependency ë²„ì „ ì¶©ëŒ

```
ImportError: cannot import name 'xxx' from 'transformers'
```

**í•´ê²°ì±…**: ì •í™•í•œ ë²„ì „ ì¬ì„¤ì¹˜
```bash
!pip uninstall -y transformers tokenizers timm
!pip install transformers==4.40.1 tokenizers==0.19.1 timm==0.9.10
```

### ğŸ“Š ë‹¤ë¥¸ Task Suites ì‹¤í–‰

```bash
# LIBERO-Object
!python experiments/robot/libero/run_libero_eval.py \
  --model_family openvla \
  --pretrained_checkpoint openvla/openvla-7b-finetuned-libero-object \
  --task_suite_name libero_object \
  --center_crop True \
  --load_in_8bit True

# LIBERO-Goal
!python experiments/robot/libero/run_libero_eval.py \
  --model_family openvla \
  --pretrained_checkpoint openvla/openvla-7b-finetuned-libero-goal \
  --task_suite_name libero_goal \
  --center_crop True \
  --load_in_8bit True

# LIBERO-10 (Long Horizon)
!python experiments/robot/libero/run_libero_eval.py \
  --model_family openvla \
  --pretrained_checkpoint openvla/openvla-7b-finetuned-libero-10 \
  --task_suite_name libero_10 \
  --center_crop True \
  --load_in_8bit True
```

### ğŸ’¡ Colab Pro íŒ

1. **GPU ì„ íƒ**: Runtime > Change runtime type > GPU (T4, V100, or A100)
2. **ë©”ëª¨ë¦¬ ì‚¬ìš© ëª¨ë‹ˆí„°ë§**:
   ```python
   !nvidia-smi
   ```
3. **ì„¸ì…˜ ìœ ì§€**: Colabì´ ìë™ìœ¼ë¡œ ì—°ê²°ì„ ëŠì§€ ì•Šë„ë¡ ì£¼ì˜
4. **ê²°ê³¼ ì €ì¥**: Google Driveì— ë§ˆìš´íŠ¸í•˜ì—¬ ë¡œê·¸ ì €ì¥
   ```python
   from google.colab import drive
   drive.mount('/content/drive')
   ```

### ğŸ“ ë…¼ë¬¸ ì¬í˜„ì„ ìœ„í•œ ê¶Œì¥ ì‚¬í•­

ë…¼ë¬¸ì˜ ê²°ê³¼ë¥¼ ì •í™•íˆ ì¬í˜„í•˜ë ¤ë©´:

- **Python**: 3.10.13
- **PyTorch**: 2.2.0
- **transformers**: 4.40.1
- **flash-attn**: 2.5.5
- **GPU**: NVIDIA A100

âš ï¸ **ì£¼ì˜**: Colabì˜ ë¬´ë£Œ T4 GPUì—ì„œëŠ” ì •í™•í•œ ì¬í˜„ì´ ì–´ë ¤ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê²°ê³¼ê°€ ì•½ê°„ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ğŸ“ ì™„ì „í•œ Colab ë…¸íŠ¸ë¶ ì˜ˆì œ

```python
# Cell 1: ì„¤ì¹˜
!git clone https://github.com/openvla/openvla.git
%cd openvla
!python experiments/robot/libero/colab_setup_libero.py

# Cell 2: GPU í™•ì¸
import torch
print(f"GPU: {torch.cuda.get_device_name(0)}")
print(f"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")

# Cell 3: Evaluation ì‹¤í–‰
!python experiments/robot/libero/run_libero_eval.py \
  --model_family openvla \
  --pretrained_checkpoint openvla/openvla-7b-finetuned-libero-spatial \
  --task_suite_name libero_spatial \
  --center_crop True \
  --load_in_8bit True \
  --num_trials_per_task 10

# Cell 4: ê²°ê³¼ í™•ì¸
!cat experiments/logs/*.txt | tail -20
```

---

## English Guide

### ğŸ¯ Overview

This guide explains how to run OpenVLA's LIBERO Simulation Benchmark Evaluations on Google Colab and solve common issues.

### âš ï¸ Common Issues

When running LIBERO evaluation on Colab, you may encounter:

1. **Tensor Size Mismatch (291 vs 290)** â­ MOST COMMON!
   - Eager attention mode has a token length calculation bug
   - Causes episodes to fail with "size of tensor a (291) must match (290)"
   - **Solution: Use SDPA (Scaled Dot Product Attention) instead!**

2. **Flash Attention 2 Compatibility**
   - Flash Attention 2 may not be supported on Colab GPUs (especially T4)
   - Installation failures due to CUDA version mismatch

3. **8-bit Quantization Errors**
   - Conflicts between `bitsandbytes` library and transformers versions
   - CUDA kernel loading failures
   - Slow performance even when working

4. **Dependency Compatibility**
   - Version mismatches in PyTorch, transformers, tokenizers
   - Conflicts with Colab's pre-installed packages

5. **Out of Memory**
   - Insufficient memory on T4 GPU (16GB) for 7B model
   - Requires ~14GB even with bfloat16

### ğŸš€ Quick Start (Run in Colab)

#### Step 1: Clone Repository and Setup

```bash
# Run in Colab notebook
!git clone https://github.com/openvla/openvla.git
%cd openvla

# Run Colab-specific setup script
!python experiments/robot/libero/colab_setup_libero.py
```

This script automatically:
- Detects GPU type (T4, V100, A100, etc.)
- Installs appropriate dependency versions
- Attempts to install Flash Attention (OK if it fails)
- Installs LIBERO and required packages
- Provides troubleshooting tips

#### Step 2: Run Evaluation

**A. For V100/A100 GPUs (without quantization):**
```bash
!python experiments/robot/libero/run_libero_eval.py \
  --model_family openvla \
  --pretrained_checkpoint openvla/openvla-7b-finetuned-libero-spatial \
  --task_suite_name libero_spatial \
  --center_crop True
```

**B. For T4 GPUs (with 8-bit quantization):**
```bash
!python experiments/robot/libero/run_libero_eval.py \
  --model_family openvla \
  --pretrained_checkpoint openvla/openvla-7b-finetuned-libero-spatial \
  --task_suite_name libero_spatial \
  --center_crop True \
  --load_in_8bit True
```

### ğŸ”§ Troubleshooting

#### â­ MOST IMPORTANT! Tensor Size Mismatch (291 vs 290)

```
Caught exception: The size of tensor a (291) must match the size of tensor b (290) at non-singleton dimension 3
```

**Cause**: OpenVLA's eager attention implementation has a token length calculation bug when combining image and text tokens.

**âœ… Solution: Use SDPA (Strongly Recommended!)**

SDPA (Scaled Dot Product Attention) offers:
- âœ… Built-in PyTorch 2.0+ feature (no extra installation needed)
- âœ… Perfect compatibility with T4 GPUs
- âœ… 70-80% of Flash Attention's speed (fast enough!)
- âœ… **No token length bugs**
- âœ… Works with 8-bit quantization

The provided `openvla_utils_colab.py` automatically tries in this order:
1. Flash Attention 2 (fastest)
2. **SDPA (recommended for T4!)** â­
3. Eager (last resort, has bugs)

**Usage**:
```bash
# Use Colab-optimized version (automatically uses SDPA)
!cp experiments/robot/openvla_utils_colab.py experiments/robot/openvla_utils.py

# Run (SDPA will be automatically selected)
!python experiments/robot/libero/run_libero_eval.py \
  --model_family openvla \
  --pretrained_checkpoint openvla/openvla-7b-finetuned-libero-spatial \
  --task_suite_name libero_spatial \
  --center_crop True \
  --load_in_8bit True
```

#### Flash Attention Errors

```
ValueError: FlashAttention only support fp16 and bf16 data type
```

**Solution**: Use the above `openvla_utils_colab.py`. It automatically falls back to SDPA (not eager!)

#### 8-bit Quantization Errors

```
RuntimeError: CUDA error: no kernel image is available for execution on the device
```
or
```
ImportError: bitsandbytes CUDA kernel loading failed
```

**Cause**: Version mismatch between `bitsandbytes` library and CUDA/transformers

**âœ… Solution 1 (Recommended)**: Reinstall compatible versions
```bash
# Install latest bitsandbytes + exact transformers version
!pip uninstall -y bitsandbytes transformers
!pip install bitsandbytes>=0.43.0
!pip install transformers==4.40.1

# Restart Colab runtime and run again
```

**âœ… Solution 2**: Use BitsAndBytesConfig (auto-handled)
```python
# openvla_utils_colab.py handles this automatically
# No manual configuration needed!
```

**Solution 3**: Run without quantization (V100/A100 only)
```bash
# Remove --load_in_8bit flag
# Warning: May fail on T4 GPUs due to insufficient memory
```

**Solution 4**: Try 4-bit quantization (less memory)
```bash
--load_in_4bit True  # Use 4-bit instead of 8-bit
```

#### CUDA Out of Memory

```
torch.cuda.OutOfMemoryError: CUDA out of memory
```

**Solution 1**: Use 8-bit quantization
```bash
--load_in_8bit True
```

**Solution 2**: Reduce number of trials
```bash
--num_trials_per_task 10  # Reduced from default 50
```

**Solution 3**: Restart runtime and clear memory
```python
import torch
torch.cuda.empty_cache()
```

#### Dependency Version Conflicts

```
ImportError: cannot import name 'xxx' from 'transformers'
```

**Solution**: Reinstall exact versions
```bash
!pip uninstall -y transformers tokenizers timm
!pip install transformers==4.40.1 tokenizers==0.19.1 timm==0.9.10
```

### ğŸ“Š Running Other Task Suites

```bash
# LIBERO-Object
!python experiments/robot/libero/run_libero_eval.py \
  --model_family openvla \
  --pretrained_checkpoint openvla/openvla-7b-finetuned-libero-object \
  --task_suite_name libero_object \
  --center_crop True \
  --load_in_8bit True

# LIBERO-Goal
!python experiments/robot/libero/run_libero_eval.py \
  --model_family openvla \
  --pretrained_checkpoint openvla/openvla-7b-finetuned-libero-goal \
  --task_suite_name libero_goal \
  --center_crop True \
  --load_in_8bit True

# LIBERO-10 (Long Horizon)
!python experiments/robot/libero/run_libero_eval.py \
  --model_family openvla \
  --pretrained_checkpoint openvla/openvla-7b-finetuned-libero-10 \
  --task_suite_name libero_10 \
  --center_crop True \
  --load_in_8bit True
```

### ğŸ’¡ Colab Pro Tips

1. **GPU Selection**: Runtime > Change runtime type > GPU (T4, V100, or A100)
2. **Monitor Memory Usage**:
   ```python
   !nvidia-smi
   ```
3. **Keep Session Alive**: Be aware that Colab may disconnect automatically
4. **Save Results**: Mount Google Drive to save logs
   ```python
   from google.colab import drive
   drive.mount('/content/drive')
   ```

### ğŸ“ Recommendations for Paper Reproduction

For exact reproduction of paper results:

- **Python**: 3.10.13
- **PyTorch**: 2.2.0
- **transformers**: 4.40.1
- **flash-attn**: 2.5.5
- **GPU**: NVIDIA A100

âš ï¸ **Note**: Exact reproduction may be difficult on Colab's free T4 GPU. Results may vary slightly.

### ğŸ“ Complete Colab Notebook Example

```python
# Cell 1: Installation
!git clone https://github.com/openvla/openvla.git
%cd openvla
!python experiments/robot/libero/colab_setup_libero.py

# Cell 2: Check GPU
import torch
print(f"GPU: {torch.cuda.get_device_name(0)}")
print(f"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")

# Cell 3: Run Evaluation
!python experiments/robot/libero/run_libero_eval.py \
  --model_family openvla \
  --pretrained_checkpoint openvla/openvla-7b-finetuned-libero-spatial \
  --task_suite_name libero_spatial \
  --center_crop True \
  --load_in_8bit True \
  --num_trials_per_task 10

# Cell 4: View Results
!cat experiments/logs/*.txt | tail -20
```

---

## ğŸ¤ ë„ì›€ì´ ë” í•„ìš”í•˜ì‹ ê°€ìš”? / Need More Help?

- GitHub Issues: https://github.com/openvla/openvla/issues
- ì´ ê°€ì´ë“œì˜ ë¬¸ì œ: ìƒˆ ì´ìŠˆë¥¼ ìƒì„±í•´ì£¼ì„¸ìš” / For issues with this guide: Create a new issue

---

## ğŸ“„ ë¼ì´ì„ ìŠ¤ / License

ì´ ê°€ì´ë“œëŠ” OpenVLA í”„ë¡œì íŠ¸ì˜ ì¼ë¶€ë¡œ MIT License í•˜ì— ë°°í¬ë©ë‹ˆë‹¤.

This guide is part of the OpenVLA project and distributed under the MIT License.
