{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ¤– OpenVLA Action Tokens ì‹¤ìŠµ (Colab + T4 GPU)\n",
    "\n",
    "**ëª©í‘œ:**\n",
    "1. ì‹¤ì œ OpenVLA 7B ëª¨ë¸ ë¡œë“œ\n",
    "2. ì´ë¯¸ì§€ + ëª…ë ¹ì–´ â†’ Action tokens ì¶”ì¶œ\n",
    "3. Token â†’ Normalized action â†’ Real action ë³€í™˜ ì „ì²´ pipeline ì´í•´\n",
    "\n",
    "**ìš”êµ¬ì‚¬í•­:**\n",
    "- GPU: T4 (16GB) âœ…\n",
    "- Runtime: Python 3\n",
    "- GPU í™œì„±í™”: Runtime â†’ Change runtime type â†’ T4 GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¦ Step 1: í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU í™•ì¸\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
    "print(\"ğŸ“¦ Installing required libraries...\")\n",
    "!pip install -q transformers torch pillow huggingface_hub\n",
    "print(\"âœ… Installation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import json\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¤– Step 2: OpenVLA ëª¨ë¸ ë¡œë“œ\n",
    "\n",
    "**ì£¼ì˜:** ì²« ì‹¤í–‰ ì‹œ ~14GB ë‹¤ìš´ë¡œë“œ (5-10ë¶„ ì†Œìš”)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForVision2Seq, AutoProcessor\n",
    "\n",
    "MODEL_ID = \"openvla/openvla-7b\"\n",
    "\n",
    "print(f\"ğŸ¤– Loading OpenVLA model: {MODEL_ID}\")\n",
    "print(\"   This may take 5-10 minutes on first run...\\n\")\n",
    "\n",
    "# Processor ë¡œë“œ\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "print(\"âœ… Processor loaded\")\n",
    "\n",
    "# ëª¨ë¸ ë¡œë“œ (bfloat16ìœ¼ë¡œ VRAM ì ˆì•½)\n",
    "vla = AutoModelForVision2Seq.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True\n",
    ").to(\"cuda\")\n",
    "\n",
    "print(\"\\nâœ… Model loaded successfully!\")\n",
    "print(f\"   Device: {vla.device}\")\n",
    "print(f\"   Dtype: {vla.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ–¼ï¸ Step 3: í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ì¤€ë¹„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°©ë²• 1: ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„±\n",
    "dummy_image = Image.new('RGB', (224, 224), color=(70, 130, 180))  # Steel blue\n",
    "dummy_image.save('/tmp/test_image.png')\n",
    "\n",
    "# ì´ë¯¸ì§€ í™•ì¸\n",
    "display(dummy_image)\n",
    "print(\"âœ… Test image created: /tmp/test_image.png\")\n",
    "\n",
    "# ë°©ë²• 2: ì‹¤ì œ ì´ë¯¸ì§€ ì—…ë¡œë“œí•˜ë ¤ë©´ ì•„ë˜ ì£¼ì„ í•´ì œ\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "# image_path = list(uploaded.keys())[0]\n",
    "# test_image = Image.open(image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Step 4: Action Prediction (ì „ì²´ Pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì…ë ¥ ì„¤ì •\n",
    "image_path = '/tmp/test_image.png'\n",
    "instruction = \"pick up the blue block\"\n",
    "unnorm_key = \"bridge_orig\"  # Bridge dataset statistics ì‚¬ìš©\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ¯ OpenVLA Action Prediction\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\n[ì…ë ¥]\")\n",
    "print(f\"  Image: {image_path}\")\n",
    "print(f\"  Instruction: {instruction}\")\n",
    "print(f\"  Dataset: {unnorm_key}\")\n",
    "\n",
    "# ì´ë¯¸ì§€ ë¡œë“œ\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "# Prompt ìƒì„± (OpenVLA í˜•ì‹)\n",
    "prompt = f\"In: What action should the robot take to {instruction.lower()}?\\nOut:\"\n",
    "print(f\"\\n  Prompt: {prompt}\")\n",
    "\n",
    "# Processorë¡œ ì…ë ¥ ì²˜ë¦¬\n",
    "inputs = processor(prompt, image).to(vla.device, dtype=vla.dtype)\n",
    "\n",
    "# Action ì˜ˆì¸¡ (ê°„ë‹¨í•œ ë°©ë²•)\n",
    "print(f\"\\n[Inference] Predicting action...\")\n",
    "with torch.no_grad():\n",
    "    action = vla.predict_action(\n",
    "        **inputs,\n",
    "        unnorm_key=unnorm_key,\n",
    "        do_sample=False  # Greedy decoding\n",
    "    )\n",
    "\n",
    "print(f\"\\nâœ… Prediction complete!\")\n",
    "print(f\"\\n[ì¶œë ¥] Real Robot Action:\")\n",
    "print(f\"  {action}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ” Step 5: Action Token ì¶”ì¶œ ë° ë¶„ì„ (í•µì‹¬!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"ğŸ” Action Token ë¶„ì„\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Step 1: ëª¨ë¸ì´ ìƒì„±í•œ token sequence ì¶”ì¶œ\n",
    "print(\"\\n[Step 1] Generating action tokens...\")\n",
    "action_dim = 7  # Bridge dataset: [6 EEF + 1 gripper]\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_ids = vla.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=action_dim,\n",
    "        do_sample=False\n",
    "    )\n",
    "\n",
    "# ë§ˆì§€ë§‰ 7ê°œ token ì¶”ì¶œ (openvla.py:90)\n",
    "action_token_ids = generated_ids[0, -action_dim:].cpu().numpy()\n",
    "\n",
    "print(f\"  Generated token IDs: {action_token_ids}\")\n",
    "print(f\"  Shape: {action_token_ids.shape}\")\n",
    "print(f\"  Vocab size: {vla.config.vocab_size}\")\n",
    "\n",
    "# Step 2: Token â†’ Normalized action [-1, 1]\n",
    "print(f\"\\n[Step 2] Token IDs â†’ Normalized actions\")\n",
    "normalized_actions = vla.action_tokenizer.decode_token_ids_to_actions(\n",
    "    action_token_ids\n",
    ")\n",
    "\n",
    "print(f\"  Normalized actions: {normalized_actions}\")\n",
    "print(f\"  Range: [{normalized_actions.min():.3f}, {normalized_actions.max():.3f}]\")\n",
    "\n",
    "# Step 3: Dataset statistics ë¡œë“œ\n",
    "print(f\"\\n[Step 3] Loading dataset statistics (unnorm_key={unnorm_key})\")\n",
    "action_stats = vla.get_action_stats(unnorm_key)\n",
    "\n",
    "q01 = np.array(action_stats[\"q01\"])\n",
    "q99 = np.array(action_stats[\"q99\"])\n",
    "mask = action_stats.get(\"mask\", np.ones_like(q01, dtype=bool))\n",
    "\n",
    "print(f\"  q01 (1% quantile): {q01}\")\n",
    "print(f\"  q99 (99% quantile): {q99}\")\n",
    "print(f\"  mask: {mask}\")\n",
    "\n",
    "# Step 4: Un-normalization (openvla.py:97-101)\n",
    "print(f\"\\n[Step 4] Un-normalization â†’ Real robot commands\")\n",
    "real_actions = 0.5 * (normalized_actions + 1) * (q99 - q01) + q01\n",
    "real_actions = np.where(mask, real_actions, normalized_actions)\n",
    "\n",
    "print(f\"  Real actions: {real_actions}\")\n",
    "\n",
    "# ê²€ì¦: predict_action() ê²°ê³¼ì™€ ë¹„êµ\n",
    "print(f\"\\n[Verification]\")\n",
    "print(f\"  predict_action() result: {action}\")\n",
    "print(f\"  Manual calculation:      {real_actions}\")\n",
    "print(f\"  Difference: {np.abs(action - real_actions)}\")\n",
    "print(f\"  âœ… Match: {np.allclose(action, real_actions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Step 6: ìƒì„¸ ë¶„ì„ ë° ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ì°¨ì›ë³„ ë¶„ì„ í…Œì´ë¸”\n",
    "dim_names = [\n",
    "    \"X-axis delta (m)\",\n",
    "    \"Y-axis delta (m)\",\n",
    "    \"Z-axis delta (m)\",\n",
    "    \"Roll delta (rad)\",\n",
    "    \"Pitch delta (rad)\",\n",
    "    \"Yaw delta (rad)\",\n",
    "    \"Gripper (0/1)\"\n",
    "]\n",
    "\n",
    "# DataFrame ìƒì„±\n",
    "df = pd.DataFrame({\n",
    "    \"Dimension\": dim_names,\n",
    "    \"Token ID\": action_token_ids,\n",
    "    \"Bin Index\": action_token_ids - (vla.config.vocab_size - 256),\n",
    "    \"Normalized\": normalized_actions,\n",
    "    \"q01\": q01,\n",
    "    \"q99\": q99,\n",
    "    \"Real Action\": real_actions,\n",
    "    \"Mask\": mask\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ğŸ“Š Action Token ìƒì„¸ ë¶„ì„\")\n",
    "print(\"=\" * 70)\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "# ì›€ì§ì„ í•´ì„\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ğŸ¤– ë¡œë´‡ ë™ì‘ í•´ì„\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i in range(6):\n",
    "    if abs(real_actions[i]) > 0.001:\n",
    "        print(f\"  {dim_names[i]:25s}: {real_actions[i]:+.6f}\")\n",
    "        if i < 3:  # XYZ\n",
    "            print(f\"    â†’ {abs(real_actions[i]*100):.2f}cm ì´ë™\")\n",
    "        else:  # Rotation\n",
    "            print(f\"    â†’ {abs(np.degrees(real_actions[i])):.2f}Â° íšŒì „\")\n",
    "\n",
    "gripper_status = \"OPEN\" if real_actions[6] > 0.5 else \"CLOSE\"\n",
    "print(f\"\\n  Gripper: {gripper_status} ({real_actions[6]:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ® Step 7: ì—¬ëŸ¬ ëª…ë ¹ì–´ë¡œ ì‹¤í—˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì—¬ëŸ¬ ëª…ë ¹ì–´ í…ŒìŠ¤íŠ¸\n",
    "instructions = [\n",
    "    \"pick up the blue block\",\n",
    "    \"move forward\",\n",
    "    \"grasp the cup\",\n",
    "    \"open the gripper\",\n",
    "    \"lift the object\"\n",
    "]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ® Multiple Instructions Test\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "results = []\n",
    "\n",
    "for inst in instructions:\n",
    "    print(f\"\\nğŸ“ Instruction: {inst}\")\n",
    "    \n",
    "    # Prompt ìƒì„±\n",
    "    prompt = f\"In: What action should the robot take to {inst.lower()}?\\nOut:\"\n",
    "    inputs = processor(prompt, image).to(vla.device, dtype=vla.dtype)\n",
    "    \n",
    "    # ì˜ˆì¸¡\n",
    "    with torch.no_grad():\n",
    "        action = vla.predict_action(\n",
    "            **inputs,\n",
    "            unnorm_key=\"bridge_orig\",\n",
    "            do_sample=False\n",
    "        )\n",
    "    \n",
    "    print(f\"   Action: {action}\")\n",
    "    results.append({\"instruction\": inst, \"action\": action})\n",
    "\n",
    "# ê²°ê³¼ ë¹„êµ\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ğŸ“Š Results Comparison\")\n",
    "print(\"=\" * 70)\n",
    "for r in results:\n",
    "    print(f\"\\n{r['instruction']:30s}\")\n",
    "    print(f\"  XYZ: [{r['action'][0]:+.4f}, {r['action'][1]:+.4f}, {r['action'][2]:+.4f}]\")\n",
    "    print(f\"  Gripper: {r['action'][6]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’¾ Step 8: Dataset Statistics ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‹¤ì œ dataset statistics ì €ì¥\n",
    "all_stats = vla.norm_stats\n",
    "\n",
    "# JSONìœ¼ë¡œ ì €ì¥\n",
    "with open('/tmp/real_dataset_statistics.json', 'w') as f:\n",
    "    # numpy arrayë¥¼ listë¡œ ë³€í™˜\n",
    "    def convert_to_serializable(obj):\n",
    "        if isinstance(obj, dict):\n",
    "            return {k: convert_to_serializable(v) for k, v in obj.items()}\n",
    "        elif isinstance(obj, (list, tuple)):\n",
    "            return [convert_to_serializable(item) for item in obj]\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        elif isinstance(obj, (np.integer, np.floating)):\n",
    "            return obj.item()\n",
    "        else:\n",
    "            return obj\n",
    "    \n",
    "    serializable_stats = convert_to_serializable(all_stats)\n",
    "    json.dump(serializable_stats, f, indent=2)\n",
    "\n",
    "print(\"âœ… Dataset statistics saved to: /tmp/real_dataset_statistics.json\")\n",
    "print(f\"\\nAvailable datasets: {list(all_stats.keys())}\")\n",
    "\n",
    "# ë‹¤ìš´ë¡œë“œ\n",
    "from google.colab import files\n",
    "files.download('/tmp/real_dataset_statistics.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ Step 9: í•™ìŠµ ìš”ì•½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"ğŸ“š í•™ìŠµ ìš”ì•½\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\"\"\n",
    "âœ… ì™„ë£Œí•œ ë‚´ìš©:\n",
    "\n",
    "1. OpenVLA 7B ëª¨ë¸ ë¡œë“œ (T4 GPU)\n",
    "   - Processor ì´ˆê¸°í™”\n",
    "   - Model ì´ˆê¸°í™” (bfloat16)\n",
    "\n",
    "2. Action Prediction Pipeline:\n",
    "   - Image + Instruction â†’ Prompt ìƒì„±\n",
    "   - Model.generate() â†’ Token IDs ìƒì„±\n",
    "   - Token IDs â†’ Normalized actions [-1, 1]\n",
    "   - Un-normalization â†’ Real robot commands\n",
    "\n",
    "3. í•µì‹¬ ê°œë…:\n",
    "   - Action Tokenization: ì—°ì†ê°’ â†’ 256 bins â†’ Token IDs\n",
    "   - BOUNDS_Q99 Normalization: (x - q01) / (q99 - q01)\n",
    "   - Un-normalization: 0.5 * (norm + 1) * (q99 - q01) + q01\n",
    "   - Mask: GripperëŠ” ì •ê·œí™” ì•ˆ í•¨\n",
    "\n",
    "4. ì‹¤ì „ ê²½í—˜:\n",
    "   - ì‹¤ì œ ëª¨ë¸ë¡œ token ì¶”ì¶œ\n",
    "   - Dataset statistics í™œìš©\n",
    "   - ì—¬ëŸ¬ ëª…ë ¹ì–´ë¡œ ì‹¤í—˜\n",
    "\n",
    "ğŸ“Œ ë‹¤ìŒ ë‹¨ê³„:\n",
    "   - RLDS ë°ì´í„° í¬ë§· ì´í•´\n",
    "   - Fine-tuning ì‹¤ìŠµ\n",
    "   - ì‹¤ì œ ë¡œë´‡ ì œì–´ (Control Frequency)\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ğŸ‰ ì‹¤ìŠµ ì™„ë£Œ!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’¡ ì¶”ê°€ ì‹¤í—˜ (ì„ íƒ)\n",
    "\n",
    "### ì‹¤ì œ ì´ë¯¸ì§€ ì—…ë¡œë“œí•´ì„œ í…ŒìŠ¤íŠ¸:\n",
    "\n",
    "```python\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "image_path = list(uploaded.keys())[0]\n",
    "\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "instruction = \"your custom instruction\"\n",
    "\n",
    "# ... (ìœ„ ì½”ë“œ ì‹¤í–‰)\n",
    "```\n",
    "\n",
    "### ë‹¤ë¥¸ Dataset ì‚¬ìš©:\n",
    "\n",
    "```python\n",
    "# bridge_orig ëŒ€ì‹  ë‹¤ë¥¸ dataset\n",
    "unnorm_key = \"fractal20220817_data\"\n",
    "action = vla.predict_action(**inputs, unnorm_key=unnorm_key)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
