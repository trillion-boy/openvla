# VLA ì½”ë”© í•™ìŠµ ë¡œë“œë§µ ğŸš€

ì´ˆë³´ìë¥¼ ìœ„í•œ OpenVLA ì½”ë“œë² ì´ìŠ¤ íƒí—˜ ê°€ì´ë“œ

---

## ğŸ¯ í•™ìŠµ ëª©í‘œ

1. **7 action tokens ì¶”ì¶œ**: ëª¨ë¸ ì¶œë ¥ â†’ ì‹¤ì œ ë¡œë´‡ ëª…ë ¹ ë³€í™˜ ê³¼ì • ì´í•´
2. **VLA êµ¬ì¡° íŒŒì•…**: Vision-Language-Action ëª¨ë¸ì˜ ë°ì´í„° íë¦„ ì´í•´
3. **TACO ì ìš© ì¤€ë¹„**: Logits ì œì–´ ì‹œ normalization ê³µê°„ ì´í•´

---

## ğŸ“š ë‹¨ê³„ë³„ í•™ìŠµ ê²½ë¡œ

### **Phase 1: Action Pipeline ì´í•´** (1-2ì¼)

ê°€ì¥ ì§ê´€ì ì´ê³  ì‹¤ìš©ì ì¸ ë¶€ë¶„ë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤.

#### Step 1.1: Token ë³€í™˜ ì‹¤ìŠµ âœ… (ê°€ì¥ ì‰¬ì›€!)
```bash
# ë°©ê¸ˆ ë§Œë“  íŒŒì¼ ì‹¤í–‰
python practice_action_tokens.py
```

**í•™ìŠµ ë‚´ìš©:**
- [ ] 7ê°œ action tokenì´ ë¬´ì—‡ì¸ì§€
- [ ] Token ID â†’ ì—°ì† ê°’ ë³€í™˜ (binning ê°œë…)
- [ ] ì •ê·œí™”/ì—­ì •ê·œí™” ìˆ˜ì‹

**í•µì‹¬ íŒŒì¼:**
- `practice_action_tokens.py` â† ë°©ê¸ˆ ë§Œë“  ì‹¤ìŠµ íŒŒì¼
- `prismatic/models/action_tokenizer.py:40-80` â† ì‹¤ì œ êµ¬í˜„

**ë””ë²„ê¹… íŒ:**
```python
# ì¤‘ê°„ê°’ ì¶œë ¥í•´ì„œ í™•ì¸í•˜ê¸°
print(f"Generated IDs shape: {generated_ids.shape}")
print(f"Last 7 tokens: {generated_ids[0, -7:]}")
print(f"Vocab size: {vla.config.vocab_size}")
```

---

#### Step 1.2: Dataset Statistics ì°¾ê¸°
```bash
# í•™ìŠµ ì‹œ ìƒì„±ë˜ëŠ” í†µê³„ íŒŒì¼ ìœ„ì¹˜ í™•ì¸
find ~/.cache/orca -name "dataset_statistics*.json" 2>/dev/null
find . -name "dataset_statistics.json" 2>/dev/null
```

**í•™ìŠµ ë‚´ìš©:**
- [ ] q01, q99ê°€ ë­”ì§€ (1% / 99% quantile)
- [ ] ì™œ mean/stdê°€ ì•„ë‹ˆë¼ quantileì„ ì“°ëŠ”ì§€ (outlier ì œê±°)
- [ ] Bridge datasetì˜ ì‹¤ì œ action ë²”ìœ„

**í•µì‹¬ íŒŒì¼:**
- `prismatic/vla/datasets/rlds/utils/data_utils.py:185-293`
  - `get_dataset_statistics()` í•¨ìˆ˜
  - `NormalizationType.BOUNDS_Q99` ì •ì˜

**ì‹¤í—˜í•´ë³´ê¸°:**
```python
import json
import numpy as np

# Statistics ë¡œë“œ
with open("path/to/dataset_statistics.json") as f:
    stats = json.load(f)

bridge_stats = stats["bridge_orig"]["action"]
print(f"q01: {bridge_stats['q01']}")
print(f"q99: {bridge_stats['q99']}")
print(f"Action range: {np.array(bridge_stats['q99']) - np.array(bridge_stats['q01'])}")

# ì •ê·œí™” ë³€í™˜ í…ŒìŠ¤íŠ¸
def normalize(action, q01, q99):
    return 2 * (action - q01) / (q99 - q01) - 1

# ì˜ˆ: Xì¶• 10cm ì´ë™ì´ ì •ê·œí™” ê³µê°„ì—ì„œ ì–¼ë§ˆì¸ì§€?
real_action = 0.10  # 10cm in meters
norm_action = normalize(real_action, bridge_stats['q01'][0], bridge_stats['q99'][0])
print(f"10cm â†’ normalized: {norm_action}")
```

---

#### Step 1.3: Inference Pipeline ë”°ë¼ê°€ê¸°
```bash
# ëª¨ë¸ ì¶”ë¡  ì˜ˆì œ ì‹¤í–‰
python experiments/bridge/verify_openvla.py
```

**í•™ìŠµ ë‚´ìš©:**
- [ ] `predict_action()` í•¨ìˆ˜ ë‚´ë¶€ íë¦„
- [ ] `unnorm_key="bridge_orig"` íŒŒë¼ë¯¸í„° ì—­í• 
- [ ] ì „ì²´ pipeline: Image â†’ Tokens â†’ Actions

**í•µì‹¬ íŒŒì¼:**
- `prismatic/models/openvla.py:61-103` â† `predict_action()` êµ¬í˜„
- `experiments/bridge/verify_openvla.py:84` â† ì‚¬ìš© ì˜ˆì œ

**ì½”ë“œ ë¦¬ë”© ìˆœì„œ:**
```python
# 1. ì…ë ¥ ì¤€ë¹„ (verify_openvla.py:70-78)
inputs = processor(prompt, image)

# 2. í† í° ìƒì„± (openvla.py:69-77)
generated_ids = self.generate(**inputs, max_new_tokens=action_dim)

# 3. Action í† í° ì¶”ì¶œ (openvla.py:84)
action_token_ids = generated_ids[0, -action_dim:]

# 4. ì •ê·œí™” action ë³µì› (openvla.py:87-89)
normalized_actions = self.action_tokenizer.decode_token_ids_to_actions(...)

# 5. Un-normalization (openvla.py:94-103)
action_stats = self.get_action_stats(unnorm_key)
actions = 0.5 * (normalized_actions + 1) * (high - low) + low
```

---

### **Phase 2: Training Pipeline ì´í•´** (2-3ì¼)

ë°ì´í„°ê°€ ì–´ë–»ê²Œ ëª¨ë¸ë¡œ ë“¤ì–´ê°€ëŠ”ì§€ ì—­ì¶”ì í•©ë‹ˆë‹¤.

#### Step 2.1: Dataset Transform ì´í•´
**í•™ìŠµ ë‚´ìš©:**
- [ ] Bridge datasetì´ ì–´ë–»ê²Œ ë³€í™˜ë˜ëŠ”ì§€
- [ ] Gripper action binarization
- [ ] EEF state vs gripper state ë¶„ë¦¬

**í•µì‹¬ íŒŒì¼:**
- `prismatic/vla/datasets/rlds/oxe/transforms.py:61-86`
  - `bridge_orig_dataset_transform()` í•¨ìˆ˜

**ì‹¤í—˜:**
```python
# transforms.pyì˜ ë³€í™˜ ë¡œì§ ë”°ë¼í•´ë³´ê¸°
import tensorflow as tf

# ì›ë³¸ ë°ì´í„° (ì˜ˆì‹œ)
raw_action = tf.constant([[0.1, -0.2, 0.05, 0.0, 0.0, 0.1, 0.6]])  # 7D

# Gripper binarization
gripper_continuous = raw_action[:, -1]  # 0.6
gripper_binary = tf.where(gripper_continuous > 0.5, 1.0, -1.0)  # â†’ 1.0 (open)

print(f"ì›ë³¸ gripper: {gripper_continuous.numpy()}")
print(f"Binary gripper: {gripper_binary.numpy()}")
```

---

#### Step 2.2: Normalization ê³¼ì • ì¶”ì 
**í•™ìŠµ ë‚´ìš©:**
- [ ] BOUNDS_Q99 ì •ê·œí™” ë°©ì‹
- [ ] Action mask (gripperëŠ” ì •ê·œí™” ì•ˆí•¨!)
- [ ] ì™œ gripperëŠ” íŠ¹ë³„ ì·¨ê¸‰í•˜ëŠ”ì§€

**í•µì‹¬ íŒŒì¼:**
- `prismatic/vla/datasets/rlds/utils/data_utils.py:61-103`
  - `normalize_action_and_proprio()` í•¨ìˆ˜
- `prismatic/vla/datasets/rlds/oxe/materialize.py:35-42`
  - `action_normalization_mask` ì„¤ì •

**ì¤‘ìš” ê°œë…:**
```python
# GripperëŠ” ì´ë¯¸ 0-1 ë²”ìœ„ë¡œ í‘œì¤€í™”ë˜ì–´ ìˆìŒ
action_normalization_mask = [True, True, True, True, True, True, False]
                             # â†‘ EEF 6ê°œ ì°¨ì›ë§Œ ì •ê·œí™”      â†‘ GripperëŠ” ê·¸ëŒ€ë¡œ

# ì •ê·œí™” (EEFë§Œ)
normalized_action[:6] = 2 * (action[:6] - q01[:6]) / (q99[:6] - q01[:6]) - 1
normalized_action[6] = action[6]  # GripperëŠ” ë³€ê²½ ì—†ìŒ
```

---

#### Step 2.3: Action Tokenization
**í•™ìŠµ ë‚´ìš©:**
- [ ] ì—°ì† ê°’ì„ discrete tokenìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ì´ìœ 
- [ ] 256 binsì˜ ì˜ë¯¸
- [ ] Vocabularyì˜ ë§ˆì§€ë§‰ 256ê°œë¥¼ ì™œ ì“°ëŠ”ì§€

**í•µì‹¬ íŒŒì¼:**
- `prismatic/models/action_tokenizer.py`
- `prismatic/vla/datasets/datasets.py:40-49` â† í•™ìŠµ ì¤‘ ì‚¬ìš©

**ì‹¤í—˜:**
```python
from prismatic.models.action_tokenizer import ActionTokenizer
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("openvla/openvla-7b")
action_tokenizer = ActionTokenizer(tokenizer)

# ì—°ì† action â†’ í† í°
action = np.array([0.5, -0.3, 0.8, 0.0, -1.0, 1.0, 0.9])
tokens = action_tokenizer(action)
print(f"Action: {action}")
print(f"Tokens: {tokens}")

# í† í° â†’ ì—°ì† action (ë³µì›)
# ... (ì‹¤ì œ token IDs í•„ìš”)
```

---

### **Phase 3: TACO ì ìš© ì¤€ë¹„** (1-2ì¼)

ì´ì œ TACOë¥¼ ì–´ë–»ê²Œ í†µí•©í• ì§€ ìƒê°í•©ë‹ˆë‹¤.

#### Step 3.1: Logits ì¶”ì¶œ ìœ„ì¹˜ íŒŒì•…
**í•™ìŠµ ë‚´ìš©:**
- [ ] Action tokensì˜ logitsê°€ ì–´ë””ì„œ ë‚˜ì˜¤ëŠ”ì§€
- [ ] ìƒì„± ê³¼ì •ì—ì„œ logits ì ‘ê·¼ ë°©ë²•
- [ ] Autoregressive generation (7ê°œ í† í°ì„ ìˆœì°¨ ìƒì„±)

**í•µì‹¬ í¬ì¸íŠ¸:**
```python
# OpenVLAëŠ” autoregressiveí•˜ê²Œ 7ê°œ í† í°ì„ ìƒì„±
# ê° stepì—ì„œ:
#   logits = model(context)[vocab_size]  # ì „ì²´ vocabularyì— ëŒ€í•œ í™•ë¥ 
#   action_logits = logits[-256:]        # ë§ˆì§€ë§‰ 256ê°œë§Œ actionìš©

# TACO ì ìš© ì‹œ:
#   1. ì–´ëŠ ì°¨ì›ì˜ í† í°ì„ ìƒì„± ì¤‘ì¸ì§€ í™•ì¸ (1/7, 2/7, ...)
#   2. í•´ë‹¹ ì°¨ì›ì˜ ëª©í‘œê°’ì„ ì •ê·œí™” ê³µê°„ìœ¼ë¡œ ë³€í™˜
#   3. Logits ì¡°ì •
```

**ì½”ë“œ ì˜ˆì‹œ (pseudocode):**
```python
# TACO ì œì•½: "Xì¶•ìœ¼ë¡œ 10cm ì´ë™"
target_real = 0.10  # meters
target_norm = normalize(target_real, q01[0], q99[0])  # â†’ ì˜ˆ: 0.35

# ì •ê·œí™” ê°’ â†’ bin index
target_bin = int((target_norm + 1) / 2 * 256)  # 0.35 â†’ bin 173

# ìƒì„± ì¤‘ logits ì¡°ì •
for step in range(7):
    logits = model(...)

    if step == 0:  # Xì¶• ì°¨ì›
        # target_bin ê·¼ì²˜ logits ê°•í™” (TACO loss)
        logits = apply_taco_constraint(logits, target_bin)

    next_token = sample(logits)
```

---

#### Step 3.2: Multi-step Generation Hook
**í•™ìŠµ ë‚´ìš©:**
- [ ] `generate()` í•¨ìˆ˜ì˜ ë‚´ë¶€ êµ¬ì¡°
- [ ] `GenerationMixin` ì»¤ìŠ¤í„°ë§ˆì´ì§•
- [ ] Logits processor ì‚¬ìš©ë²•

**ì°¸ê³  íŒŒì¼:**
- HuggingFace Transformersì˜ `generation/utils.py`
- `LogitsProcessor` í´ë˜ìŠ¤ ìƒì†

**ì˜ˆì œ:**
```python
from transformers import LogitsProcessor

class TACOLogitsProcessor(LogitsProcessor):
    def __init__(self, constraints, action_tokenizer, stats):
        self.constraints = constraints
        self.action_tokenizer = action_tokenizer
        self.stats = stats
        self.current_action_dim = 0

    def __call__(self, input_ids, scores):
        # í˜„ì¬ ì–´ëŠ action ì°¨ì›ì„ ìƒì„± ì¤‘ì¸ì§€ ì¶”ì 
        if self.current_action_dim < 7:
            # í•´ë‹¹ ì°¨ì›ì˜ ì œì•½ ì ìš©
            constraint = self.constraints[self.current_action_dim]
            scores = self.apply_constraint(scores, constraint)
            self.current_action_dim += 1
        return scores

# ì‚¬ìš©
vla.generate(
    **inputs,
    logits_processor=[TACOLogitsProcessor(...)],
)
```

---

## ğŸ” í•µì‹¬ íŒŒì¼ ìš”ì•½

### **Inference (ì¶”ë¡ )**
| íŒŒì¼ | ì—­í•  | ì¤‘ìš”ë„ |
|------|------|--------|
| `prismatic/models/openvla.py` | `predict_action()` - ì „ì²´ ì¶”ë¡  pipeline | â­â­â­â­â­ |
| `prismatic/models/action_tokenizer.py` | Token â†” Action ë³€í™˜ | â­â­â­â­â­ |
| `experiments/bridge/verify_openvla.py` | ì‚¬ìš© ì˜ˆì œ | â­â­â­â­ |

### **Training (í•™ìŠµ)**
| íŒŒì¼ | ì—­í•  | ì¤‘ìš”ë„ |
|------|------|--------|
| `prismatic/vla/datasets/datasets.py` | Dataset loading + action tokenization | â­â­â­â­ |
| `prismatic/vla/datasets/rlds/utils/data_utils.py` | Normalization + statistics | â­â­â­â­â­ |
| `prismatic/vla/datasets/rlds/oxe/transforms.py` | Dataset-specific transforms | â­â­â­ |
| `prismatic/vla/datasets/rlds/oxe/materialize.py` | Dataset configs | â­â­â­ |

### **Configuration**
| íŒŒì¼ | ì—­í•  | ì¤‘ìš”ë„ |
|------|------|--------|
| `prismatic/vla/datasets/rlds/oxe/configs.py` | `bridge_orig` ë“± ì„¤ì • | â­â­â­â­ |
| `prismatic/vla/datasets/rlds/oxe/mixtures.py` | Multi-dataset mixing | â­â­ |

---

## ğŸ“ í•™ìŠµ ì²´í¬ë¦¬ìŠ¤íŠ¸

### Week 1: Action Pipeline
- [ ] `practice_action_tokens.py` ì‹¤í–‰ ì„±ê³µ
- [ ] ì—°ìŠµ ë¬¸ì œ 1, 2 í’€ì´
- [ ] Dataset statistics JSON íŒŒì¼ ì°¾ê¸°
- [ ] `verify_openvla.py` ì½”ë“œ ë¦¬ë”©
- [ ] ì§ì ‘ ì´ë¯¸ì§€ë¡œ ì¶”ë¡  ì‹¤í–‰

### Week 2: Training Pipeline
- [ ] `bridge_orig_dataset_transform()` ì´í•´
- [ ] Normalization ìˆ˜ì‹ ì†ìœ¼ë¡œ ê³„ì‚°
- [ ] Action tokenization ì‹¤í—˜
- [ ] Gripper íŠ¹ìˆ˜ ì²˜ë¦¬ ì´ìœ  ì„¤ëª… ê°€ëŠ¥

### Week 3: TACO Integration
- [ ] Logits processor êµ¬í˜„
- [ ] ì •ê·œí™” ê³µê°„ì—ì„œ ì œì•½ ê±¸ê¸°
- [ ] Multi-step generation hook
- [ ] ê°„ë‹¨í•œ TACO ì œì•½ í…ŒìŠ¤íŠ¸

---

## ğŸ’¡ ìì£¼ í•˜ëŠ” ì‹¤ìˆ˜

### 1. **ì •ê·œí™” ê³µê°„ í˜¼ë™**
âŒ ì˜ëª»ëœ ì˜ˆ:
```python
# "10cm ì´ë™" ì œì•½ì„ ì‹¤ì œ ê°’ìœ¼ë¡œ ê±¸ê¸°
target = 0.10  # meters
logits = apply_constraint(logits, target)  # ğŸš« í‹€ë¦¼!
```

âœ… ì˜¬ë°”ë¥¸ ì˜ˆ:
```python
# ë¨¼ì € ì •ê·œí™” ê³µê°„ìœ¼ë¡œ ë³€í™˜
target_real = 0.10
target_norm = 2 * (target_real - q01) / (q99 - q01) - 1
target_bin = int((target_norm + 1) / 2 * 256)
logits = apply_constraint(logits, target_bin)  # âœ… ë§ìŒ!
```

### 2. **Gripper ì •ê·œí™”**
âŒ ì˜ëª»ëœ ì˜ˆ:
```python
# Gripperë„ [-1, 1]ë¡œ ì •ê·œí™”í•œë‹¤ê³  ì°©ê°
normalized_gripper = (gripper - q01[6]) / (q99[6] - q01[6])  # ğŸš« í‹€ë¦¼!
```

âœ… ì˜¬ë°”ë¥¸ ì˜ˆ:
```python
# GripperëŠ” ì´ë¯¸ [0, 1] ë˜ëŠ” {-1, 1} (binary)
# ì •ê·œí™” í•˜ì§€ ì•ŠìŒ!
normalized_gripper = gripper  # âœ… ê·¸ëŒ€ë¡œ ì‚¬ìš©
```

### 3. **Token ID ë²”ìœ„**
âŒ ì˜ëª»ëœ ì˜ˆ:
```python
# Action tokensì´ vocabulary ì•ë¶€ë¶„ì— ìˆë‹¤ê³  ì°©ê°
action_token_ids = generated_ids[0, :7]  # ğŸš« í‹€ë¦¼!
```

âœ… ì˜¬ë°”ë¥¸ ì˜ˆ:
```python
# ë§ˆì§€ë§‰ 256ê°œê°€ action tokens
# ìƒì„±ëœ sequenceì˜ ë§ˆì§€ë§‰ 7ê°œë¥¼ ì¶”ì¶œ
action_token_ids = generated_ids[0, -7:]  # âœ… ë§ìŒ!
```

---

## ğŸ“– ì¶”ê°€ í•™ìŠµ ìë£Œ

### Paper
- **OpenVLA**: "Open-Source Vision-Language-Action Models"
- **RT-1**: "Robotics Transformer" (action tokenization ê¸°ë²•)
- **Octo**: "Open X-Embodiment" (normalization ë°©ë²•ë¡ )

### Code Reference
- HuggingFace Transformers: `generation/utils.py`
- TACO ì›ë³¸ êµ¬í˜„ (ìˆë‹¤ë©´ ë§í¬)

### Debug ëª…ë ¹ì–´
```bash
# ëª¨ë¸ êµ¬ì¡° í™•ì¸
python -c "from prismatic.models import load_vla; vla = load_vla('openvla/openvla-7b'); print(vla)"

# Tokenizer vocab size í™•ì¸
python -c "from transformers import AutoTokenizer; t = AutoTokenizer.from_pretrained('openvla/openvla-7b'); print(f'Vocab: {len(t)}')"

# Dataset stats í™•ì¸
find . -name "dataset_statistics.json" -exec cat {} \; | python -m json.tool
```

---

## ğŸš€ ë‹¤ìŒ ë‹¨ê³„

ì´ ë¡œë“œë§µì„ ì™„ë£Œí•˜ë©´:
1. VLAì˜ ì „ì²´ ë°ì´í„° íë¦„ ì´í•´ ì™„ë£Œ
2. TACO í†µí•©ì„ ìœ„í•œ ì½”ë“œ ìˆ˜ì • ìœ„ì¹˜ íŒŒì•…
3. ì •ê·œí™” ê³µê°„ì—ì„œì˜ ì œì•½ ì„¤ê³„ ê°€ëŠ¥

**ë©˜í† ì™€ ë…¼ì˜í•  ì£¼ì œ:**
- TACO lossë¥¼ ì–´ëŠ ë‹¨ê³„ì—ì„œ ì ìš©í• ì§€
- Multi-step generationì—ì„œ autoregressive TACO
- ì‹¤í—˜ ì„¤ê³„ (ì–´ë–¤ taskë¡œ ê²€ì¦í• ì§€)

---

Good luck! ğŸ‰
