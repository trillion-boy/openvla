"""
OpenVLA Action Token ì²˜ë¦¬ ì‹¤ìŠµ

ëª©í‘œ: OpenVLAì˜ action ì²˜ë¦¬ pipeline ì´í•´
1. ëª¨ë¸ì´ ìƒì„±í•œ 7ê°œ token IDs ì¶”ì¶œ
2. Token â†’ ì •ê·œí™”ëœ action [-1, 1] ë³€í™˜
3. Un-normalization â†’ ì‹¤ì œ ë¡œë´‡ ëª…ë ¹ ë³€í™˜

ê¸°ë°˜ ì½”ë“œ:
- prismatic/models/vlas/openvla.py (predict_action í•¨ìˆ˜)
- prismatic/models/action_tokenizer.py (ActionTokenizer í´ë˜ìŠ¤)
- prismatic/vla/datasets/rlds/utils/data_utils.py (ì •ê·œí™” ê´€ë ¨)
"""

import numpy as np
import torch
from pathlib import Path
from PIL import Image


# ============================================================
# Step 1: ActionTokenizer ì´í•´
# ============================================================

class SimpleActionTokenizer:
    """
    OpenVLAì˜ ActionTokenizer ê°„ì†Œí™” ë²„ì „

    ê¸°ë°˜: prismatic/models/action_tokenizer.py:40-88

    í•µì‹¬ ê°œë…:
    - ì—°ì† action ê°’ [-1, 1]ì„ 256ê°œì˜ binìœ¼ë¡œ discretize
    - ê° binì€ í•˜ë‚˜ì˜ token IDì— ë§¤í•‘
    - Vocabularyì˜ ë§ˆì§€ë§‰ 256ê°œë¥¼ action tokensë¡œ ì‚¬ìš©
    """

    def __init__(self, vocab_size: int = 32000, n_bins: int = 256):
        self.vocab_size = vocab_size
        self.n_bins = n_bins

        # Bin ê²½ê³„: [-1, 1] ë²”ìœ„ë¥¼ 256ê°œë¡œ ë¶„í• 
        # ê¸°ë°˜: action_tokenizer.py:48-49
        self.bins = np.linspace(-1, 1, n_bins + 1)

        # ê° binì˜ ì¤‘ì‹¬ê°’ (ì—­ë³€í™˜ ì‹œ ì‚¬ìš©)
        # ê¸°ë°˜: action_tokenizer.py:50
        self.bin_centers = (self.bins[:-1] + self.bins[1:]) / 2.0

        print(f"=== ActionTokenizer ì´ˆê¸°í™” ===")
        print(f"Vocab size: {vocab_size}")
        print(f"Number of bins: {n_bins}")
        print(f"Bin range: [{self.bins[0]}, {self.bins[-1]}]")
        print(f"Bin width: {self.bins[1] - self.bins[0]:.4f}")

    def encode(self, action: np.ndarray) -> np.ndarray:
        """
        ì—°ì† action â†’ token IDs

        ê¸°ë°˜: action_tokenizer.py:68-73
        """
        # Clipping
        action = np.clip(action, -1.0, 1.0)

        # Discretization
        discretized = np.digitize(action, self.bins) - 1
        discretized = np.clip(discretized, 0, self.n_bins - 1)

        # Token ID ë³€í™˜ (vocabì˜ ë§ˆì§€ë§‰ 256ê°œ ì‚¬ìš©)
        token_ids = self.vocab_size - self.n_bins + discretized

        return token_ids

    def decode_token_ids_to_actions(self, token_ids: np.ndarray) -> np.ndarray:
        """
        Token IDs â†’ ì •ê·œí™”ëœ ì—°ì† action

        ê¸°ë°˜: action_tokenizer.py:83-88

        Args:
            token_ids: [action_dim] shapeì˜ token IDs

        Returns:
            actions: [action_dim] shapeì˜ ì •ê·œí™” action [-1, 1]
        """
        # Token IDë¥¼ bin indexë¡œ ì—­ë³€í™˜
        discretized_actions = token_ids - (self.vocab_size - self.n_bins)

        # Safety clipping
        discretized_actions = np.clip(discretized_actions, 0, self.n_bins - 1)

        # Bin index â†’ ì—°ì† ê°’ (bin ì¤‘ì‹¬ê°’ ì‚¬ìš©)
        continuous_actions = self.bin_centers[discretized_actions]

        return continuous_actions


# ============================================================
# Step 2: Un-normalization ì´í•´
# ============================================================

def unnormalize_actions(normalized_actions: np.ndarray,
                       action_stats: dict) -> np.ndarray:
    """
    ì •ê·œí™”ëœ action [-1, 1] â†’ ì‹¤ì œ ë¡œë´‡ ëª…ë ¹

    ê¸°ë°˜: prismatic/models/vlas/openvla.py:94-101

    OpenVLAëŠ” BOUNDS_Q99 ì •ê·œí™” ì‚¬ìš©:
    - Forward:  norm = 2 * (action - q01) / (q99 - q01) - 1
    - Backward: action = 0.5 * (norm + 1) * (q99 - q01) + q01

    ì°¸ê³ : prismatic/vla/datasets/rlds/utils/data_utils.py:49-54
          NormalizationType.BOUNDS_Q99
    """
    action_low = np.array(action_stats["q01"])
    action_high = np.array(action_stats["q99"])

    # Un-normalization ê³µì‹ (openvla.py:97-101)
    real_actions = 0.5 * (normalized_actions + 1) * (action_high - action_low) + action_low

    # Mask ì ìš© (ì¼ë¶€ ì°¨ì›ì€ ì •ê·œí™” ì•ˆ í•¨)
    # ì˜ˆ: GripperëŠ” ì´ë¯¸ [0, 1] ë²”ìœ„ë¼ ì •ê·œí™” ì•ˆ í•¨
    mask = action_stats.get("mask", np.ones_like(action_low, dtype=bool))
    real_actions = np.where(mask, real_actions, normalized_actions)

    return real_actions


# ============================================================
# Step 3: ì „ì²´ Pipeline ì˜ˆì œ
# ============================================================

def demonstrate_action_pipeline():
    """
    OpenVLAì˜ action ì²˜ë¦¬ ì „ì²´ íë¦„ ì‹œì—°

    ê¸°ë°˜: prismatic/models/vlas/openvla.py:84-103
    """
    print("\n" + "=" * 60)
    print("OpenVLA Action Processing Pipeline")
    print("=" * 60)

    # ì˜ˆì œ ë°ì´í„°
    vocab_size = 32000
    action_dim = 7  # Bridge dataset: [6 EEF + 1 gripper]

    # Step 1: ëª¨ë¸ì´ ìƒì„±í•œ token IDs (ì˜ˆì‹œ)
    # ì‹¤ì œë¡œëŠ” vla.generate()ì˜ ê²°ê³¼
    print("\n[Step 1] ëª¨ë¸ ìƒì„± ê²°ê³¼")
    generated_ids = np.array([31800, 31850, 31900, 31750, 31780, 31820, 31950])
    print(f"Generated token IDs (ë§ˆì§€ë§‰ 7ê°œ): {generated_ids}")
    print(f"  - ë²”ìœ„: [{generated_ids.min()}, {generated_ids.max()}]")
    print(f"  - Vocab size: {vocab_size}")
    print(f"  - Action token ë²”ìœ„: [{vocab_size - 256}, {vocab_size}]")

    # Step 2: Token â†’ ì •ê·œí™” action
    print("\n[Step 2] Token IDs â†’ ì •ê·œí™” action")
    tokenizer = SimpleActionTokenizer(vocab_size=vocab_size)
    normalized_actions = tokenizer.decode_token_ids_to_actions(generated_ids)
    print(f"ì •ê·œí™” actions: {normalized_actions}")
    print(f"  - ë²”ìœ„: [{normalized_actions.min():.3f}, {normalized_actions.max():.3f}]")

    # Step 3: Dataset statistics ë¡œë“œ (ì˜ˆì‹œ)
    print("\n[Step 3] Dataset statistics")
    # ì‹¤ì œë¡œëŠ” dataset_statistics.jsonì—ì„œ ë¡œë“œ
    # ê¸°ë°˜: prismatic/vla/datasets/rlds/utils/data_utils.py:185-293
    bridge_stats = {
        "q01": np.array([-0.4, -0.35, -0.5, -0.3, -0.25, -0.3, 0.0]),
        "q99": np.array([0.45, 0.38, 0.52, 0.32, 0.28, 0.32, 1.0]),
        "mask": np.array([True, True, True, True, True, True, False])
    }
    print(f"q01 (1% quantile):  {bridge_stats['q01']}")
    print(f"q99 (99% quantile): {bridge_stats['q99']}")
    print(f"Action range:       {bridge_stats['q99'] - bridge_stats['q01']}")

    # Step 4: Un-normalization
    print("\n[Step 4] Un-normalization â†’ ì‹¤ì œ ë¡œë´‡ ëª…ë ¹")
    real_actions = unnormalize_actions(normalized_actions, bridge_stats)
    print(f"ì‹¤ì œ actions: {real_actions}")
    print(f"\nì°¨ì›ë³„ í•´ì„ (Bridge dataset):")
    print(f"  [0] X-axis delta (m):        {real_actions[0]:+.4f}")
    print(f"  [1] Y-axis delta (m):        {real_actions[1]:+.4f}")
    print(f"  [2] Z-axis delta (m):        {real_actions[2]:+.4f}")
    print(f"  [3] Roll delta (rad):        {real_actions[3]:+.4f}")
    print(f"  [4] Pitch delta (rad):       {real_actions[4]:+.4f}")
    print(f"  [5] Yaw delta (rad):         {real_actions[5]:+.4f}")
    print(f"  [6] Gripper (0=close, 1=open): {real_actions[6]:.2f}")


# ============================================================
# Step 4: ì—­ë³€í™˜ ê²€ì¦
# ============================================================

def verify_normalization_inverse():
    """
    ì •ê·œí™” â†” ì—­ì •ê·œí™”ê°€ ì˜¬ë°”ë¥¸ ì—­í•¨ìˆ˜ì¸ì§€ ê²€ì¦

    ê¸°ë°˜: data_utils.pyì˜ ì •ê·œí™” ê³µì‹
    """
    print("\n" + "=" * 60)
    print("ì •ê·œí™”/ì—­ì •ê·œí™” ê²€ì¦")
    print("=" * 60)

    # ì˜ˆì œ statistics
    q01, q99 = -0.4, 0.45

    # ì›ë³¸ action
    original_action = 0.10  # 10cm
    print(f"\nì›ë³¸ action: {original_action:.3f} m")

    # Forward: ì •ê·œí™”
    normalized = 2 * (original_action - q01) / (q99 - q01) - 1
    print(f"ì •ê·œí™” í›„:   {normalized:.3f}")

    # Backward: ì—­ì •ê·œí™”
    recovered = 0.5 * (normalized + 1) * (q99 - q01) + q01
    print(f"ë³µì› í›„:     {recovered:.3f} m")

    # ê²€ì¦
    error = abs(original_action - recovered)
    print(f"ë³µì› ì˜¤ì°¨:   {error:.6f}")
    assert error < 1e-6, "ì •ê·œí™”/ì—­ì •ê·œí™” ì˜¤ì°¨ ë°œìƒ!"
    print("âœ… ê²€ì¦ í†µê³¼!")


# ============================================================
# Step 5: ì‹¤ì œ ì½”ë“œ ìœ„ì¹˜ ì°¸ê³ 
# ============================================================

def print_code_references():
    """
    OpenVLA ì½”ë“œë² ì´ìŠ¤ì˜ ê´€ë ¨ íŒŒì¼ë“¤
    """
    print("\n" + "=" * 60)
    print("OpenVLA ì½”ë“œ ì°¸ê³ ")
    print("=" * 60)

    references = [
        {
            "íŒŒì¼": "prismatic/models/vlas/openvla.py",
            "í•¨ìˆ˜/í´ë˜ìŠ¤": "OpenVLA.predict_action()",
            "ë¼ì¸": "84-103",
            "ì„¤ëª…": "ì „ì²´ action ìƒì„± pipeline"
        },
        {
            "íŒŒì¼": "prismatic/models/action_tokenizer.py",
            "í•¨ìˆ˜/í´ë˜ìŠ¤": "ActionTokenizer",
            "ë¼ì¸": "40-88",
            "ì„¤ëª…": "Token â†” Action ë³€í™˜"
        },
        {
            "íŒŒì¼": "prismatic/vla/datasets/rlds/utils/data_utils.py",
            "í•¨ìˆ˜/í´ë˜ìŠ¤": "get_dataset_statistics()",
            "ë¼ì¸": "185-293",
            "ì„¤ëª…": "Dataset statistics ê³„ì‚°/ë¡œë“œ"
        },
        {
            "íŒŒì¼": "prismatic/vla/datasets/rlds/utils/data_utils.py",
            "í•¨ìˆ˜/í´ë˜ìŠ¤": "NormalizationType.BOUNDS_Q99",
            "ë¼ì¸": "49-54",
            "ì„¤ëª…": "ì •ê·œí™” ë°©ì‹ ì •ì˜"
        },
        {
            "íŒŒì¼": "prismatic/vla/datasets/rlds/oxe/configs.py",
            "í•¨ìˆ˜/í´ë˜ìŠ¤": "OXE_DATASET_CONFIGS['bridge_orig']",
            "ë¼ì¸": "79-85",
            "ì„¤ëª…": "Bridge dataset ì„¤ì •"
        },
    ]

    for ref in references:
        print(f"\nğŸ“„ {ref['íŒŒì¼']}:{ref['ë¼ì¸']}")
        print(f"   {ref['í•¨ìˆ˜/í´ë˜ìŠ¤']}")
        print(f"   â†’ {ref['ì„¤ëª…']}")


# ============================================================
# ì—°ìŠµ ë¬¸ì œ
# ============================================================

def exercise_1_token_conversion():
    """
    ì—°ìŠµ 1: Token ID â†” Action ë³€í™˜

    ë¬¸ì œ: ë‹¤ìŒ token IDê°€ ì–´ë–¤ action ê°’ìœ¼ë¡œ ë³€í™˜ë˜ëŠ”ì§€ ê³„ì‚°í•˜ì„¸ìš”.
    """
    print("\n" + "=" * 60)
    print("ì—°ìŠµ ë¬¸ì œ 1: Token â†’ Action ë³€í™˜")
    print("=" * 60)

    vocab_size = 32000
    n_bins = 256

    # Token ID ì˜ˆì‹œ
    token_id = 31872  # vocab_size - 128

    print(f"\nì£¼ì–´ì§„ ê°’:")
    print(f"  Vocab size: {vocab_size}")
    print(f"  Bins: {n_bins}")
    print(f"  Token ID: {token_id}")

    # TODO: ì—¬ê¸°ì„œë¶€í„° ê³„ì‚°
    # íŒíŠ¸ 1: bin_index = token_id - (vocab_size - n_bins)
    # íŒíŠ¸ 2: bin_centers = linspace(-1, 1, n_bins)ì˜ ì¤‘ì‹¬ê°’ë“¤

    bin_index = token_id - (vocab_size - n_bins)
    bins = np.linspace(-1, 1, n_bins + 1)
    bin_centers = (bins[:-1] + bins[1:]) / 2.0
    action_value = bin_centers[bin_index]

    print(f"\nì •ë‹µ:")
    print(f"  Bin index: {bin_index}")
    print(f"  Action value: {action_value:.4f}")


def exercise_2_unnormalization():
    """
    ì—°ìŠµ 2: Un-normalization ê³„ì‚°

    ë¬¸ì œ: ì •ê·œí™”ëœ actionì„ ì‹¤ì œ ë¡œë´‡ ëª…ë ¹ìœ¼ë¡œ ë³€í™˜í•˜ì„¸ìš”.
    """
    print("\n" + "=" * 60)
    print("ì—°ìŠµ ë¬¸ì œ 2: Un-normalization")
    print("=" * 60)

    # ì£¼ì–´ì§„ ê°’
    normalized_actions = np.array([0.5, -0.3, 0.8, 0.0, -1.0, 1.0, 0.9])
    q01 = np.array([-0.4, -0.35, -0.5, -0.3, -0.25, -0.3, 0.0])
    q99 = np.array([0.45, 0.38, 0.52, 0.32, 0.28, 0.32, 1.0])

    print(f"\nì£¼ì–´ì§„ ê°’:")
    print(f"  ì •ê·œí™” actions: {normalized_actions}")
    print(f"  q01: {q01}")
    print(f"  q99: {q99}")

    # TODO: Un-normalization ìˆ˜ì‹ ì ìš©
    # ê³µì‹: action = 0.5 * (norm + 1) * (q99 - q01) + q01

    real_actions = 0.5 * (normalized_actions + 1) * (q99 - q01) + q01

    print(f"\nì •ë‹µ:")
    print(f"  ì‹¤ì œ actions: {real_actions}")
    print(f"\nê²€ì¦ (ì°¨ì›ë³„):")
    for i in range(7):
        print(f"    Dim {i}: norm={normalized_actions[i]:+.2f} â†’ real={real_actions[i]:+.4f}")


# ============================================================
# Main
# ============================================================

if __name__ == "__main__":
    print("=" * 60)
    print("OpenVLA Action Token ì²˜ë¦¬ ì‹¤ìŠµ")
    print("=" * 60)

    # ì „ì²´ pipeline ì‹œì—°
    demonstrate_action_pipeline()

    # ì •ê·œí™” ê²€ì¦
    verify_normalization_inverse()

    # ì½”ë“œ ì°¸ê³  ì •ë³´
    print_code_references()

    # ì—°ìŠµ ë¬¸ì œ
    exercise_1_token_conversion()
    exercise_2_unnormalization()

    print("\n" + "=" * 60)
    print("ì‹¤ìŠµ ì™„ë£Œ!")
    print("=" * 60)
    print("\në‹¤ìŒ ë‹¨ê³„:")
    print("  1. OpenVLA ì‹¤ì œ ì½”ë“œ ì½ê¸° (ìœ„ ì°¸ê³  íŒŒì¼ë“¤)")
    print("  2. Dataset statistics íŒŒì¼ ì°¾ê¸°")
    print("  3. ì‹¤ì œ ëª¨ë¸ë¡œ ì¶”ë¡  ì‹¤í–‰í•´ë³´ê¸°")
