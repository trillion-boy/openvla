"""
ì‹¤ì œ OpenVLA ëª¨ë¸ë¡œ Action Tokens ì–»ê¸°

ìš”êµ¬ì‚¬í•­:
- GPU (CUDA)
- ìµœì†Œ 16GB VRAM
- transformers, torch, PIL ë¼ì´ë¸ŒëŸ¬ë¦¬

ì‹¤í–‰ ì „:
pip install transformers torch pillow huggingface_hub
"""

import torch
import numpy as np
from PIL import Image
from pathlib import Path


def load_openvla_model(model_id: str = "openvla/openvla-7b"):
    """
    OpenVLA ëª¨ë¸ ë¡œë“œ

    ê¸°ë°˜: prismatic/models/load.py
    """
    print("=" * 70)
    print("ğŸ¤– OpenVLA ëª¨ë¸ ë¡œë“œ")
    print("=" * 70)

    try:
        from transformers import AutoModelForVision2Seq, AutoProcessor
    except ImportError:
        print("âŒ transformersê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
        print("   pip install transformers torch pillow")
        return None, None

    print(f"\nëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘: {model_id}")
    print("(ì²« ì‹¤í–‰ ì‹œ ~14GB ë‹¤ìš´ë¡œë“œ, ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")

    # Processor ë¡œë“œ
    processor = AutoProcessor.from_pretrained(
        model_id,
        trust_remote_code=True
    )

    # ëª¨ë¸ ë¡œë“œ
    device = "cuda" if torch.cuda.is_available() else "cpu"
    if device == "cpu":
        print("\nâš ï¸  GPUë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¡œ ì‹¤í–‰í•˜ë©´ ë§¤ìš° ëŠë¦½ë‹ˆë‹¤!")

    vla = AutoModelForVision2Seq.from_pretrained(
        model_id,
        torch_dtype=torch.bfloat16 if device == "cuda" else torch.float32,
        low_cpu_mem_usage=True,
        trust_remote_code=True
    ).to(device)

    print(f"\nâœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ! (Device: {device})")

    return vla, processor


def predict_action_with_tokens(
    vla,
    processor,
    image_path: str,
    instruction: str,
    unnorm_key: str = "bridge_orig"
):
    """
    ì‹¤ì œ OpenVLA ëª¨ë¸ë¡œ action ì˜ˆì¸¡ + token ì¶”ì¶œ

    ê¸°ë°˜: prismatic/models/vlas/openvla.py:50-103
    """
    print("\n" + "=" * 70)
    print("ğŸ¯ Action Prediction")
    print("=" * 70)

    # ì…ë ¥ ì¤€ë¹„
    print(f"\n[ì…ë ¥]")
    print(f"  ì´ë¯¸ì§€: {image_path}")
    print(f"  ëª…ë ¹ì–´: {instruction}")

    image = Image.open(image_path).convert("RGB")
    prompt = f"In: What action should the robot take to {instruction.lower()}?\nOut:"

    # Processorë¡œ ì…ë ¥ ì²˜ë¦¬
    inputs = processor(prompt, image).to(vla.device, dtype=vla.dtype)

    # Action ì˜ˆì¸¡ (predict_action ë©”ì„œë“œ ì‚¬ìš©)
    print(f"\n[Step 1] ëª¨ë¸ ì¶”ë¡  ì¤‘...")
    with torch.no_grad():
        # ë°©ë²• 1: predict_action ì‚¬ìš© (un-normalization í¬í•¨)
        action = vla.predict_action(**inputs, unnorm_key=unnorm_key, do_sample=False)

    print(f"  âœ… ì˜ˆì¸¡ ì™„ë£Œ!")
    print(f"  ì‹¤ì œ ë¡œë´‡ ëª…ë ¹: {action}")

    # ë°©ë²• 2: ìˆ˜ë™ìœ¼ë¡œ tokens ì¶”ì¶œ
    print(f"\n[Step 2] Token IDs ìˆ˜ë™ ì¶”ì¶œ...")
    with torch.no_grad():
        # Generateë¡œ ì§ì ‘ ìƒì„±
        action_dim = 7
        generated_ids = vla.generate(
            **inputs,
            max_new_tokens=action_dim,
            do_sample=False
        )

        # ë§ˆì§€ë§‰ 7ê°œ token ì¶”ì¶œ (openvla.py:90)
        action_token_ids = generated_ids[0, -action_dim:].cpu().numpy()

    print(f"  ìƒì„±ëœ token IDs: {action_token_ids}")

    # Token â†’ Normalized action
    normalized_actions = vla.action_tokenizer.decode_token_ids_to_actions(action_token_ids)
    print(f"  ì •ê·œí™” actions: {normalized_actions}")

    # Un-normalization ìˆ˜ë™ ìˆ˜í–‰
    action_stats = vla.get_action_stats(unnorm_key)
    q01 = np.array(action_stats["q01"])
    q99 = np.array(action_stats["q99"])
    mask = action_stats.get("mask", np.ones_like(q01, dtype=bool))

    manual_action = 0.5 * (normalized_actions + 1) * (q99 - q01) + q01
    manual_action = np.where(mask, manual_action, normalized_actions)

    print(f"  ìˆ˜ë™ un-normalization: {manual_action}")

    # ê²€ì¦
    print(f"\n[ê²€ì¦]")
    print(f"  predict_action() ê²°ê³¼: {action}")
    print(f"  ìˆ˜ë™ ê³„ì‚° ê²°ê³¼:        {manual_action}")
    print(f"  ì°¨ì´: {np.abs(action - manual_action)}")

    return {
        "action": action,
        "action_token_ids": action_token_ids,
        "normalized_actions": normalized_actions,
        "manual_action": manual_action
    }


def interactive_demo(vla, processor):
    """ëŒ€í™”í˜• ë°ëª¨"""
    print("\n" + "=" * 70)
    print("ğŸ® ëŒ€í™”í˜• Action ì˜ˆì¸¡")
    print("=" * 70)

    print("\nì‚¬ìš©ë²•:")
    print("  1. ì´ë¯¸ì§€ ê²½ë¡œ ì…ë ¥")
    print("  2. ë¡œë´‡ ëª…ë ¹ì–´ ì…ë ¥")
    print("  3. Action tokens í™•ì¸!")
    print("  (ì¢…ë£Œ: Ctrl+C)")

    try:
        while True:
            print("\n" + "-" * 70)
            image_path = input("ì´ë¯¸ì§€ ê²½ë¡œ: ").strip()
            if not image_path or not Path(image_path).exists():
                print("âŒ ìœ íš¨í•œ ì´ë¯¸ì§€ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”")
                continue

            instruction = input("ë¡œë´‡ ëª…ë ¹ì–´: ").strip()
            if not instruction:
                print("âŒ ëª…ë ¹ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”")
                continue

            result = predict_action_with_tokens(
                vla, processor, image_path, instruction
            )

            print("\nâœ… ì˜ˆì¸¡ ì™„ë£Œ!")

    except KeyboardInterrupt:
        print("\n\nì¢…ë£Œí•©ë‹ˆë‹¤.")


# ============================================================
# ì˜ˆì œ ì‹¤í–‰
# ============================================================

def example_with_dummy_image():
    """ë”ë¯¸ ì´ë¯¸ì§€ë¡œ í…ŒìŠ¤íŠ¸"""
    print("\n" + "=" * 70)
    print("ì˜ˆì œ: ë”ë¯¸ ì´ë¯¸ì§€ë¡œ í…ŒìŠ¤íŠ¸")
    print("=" * 70)

    # ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„±
    dummy_image = Image.new('RGB', (224, 224), color='blue')
    dummy_path = "/tmp/dummy_image.png"
    dummy_image.save(dummy_path)

    print(f"\në”ë¯¸ ì´ë¯¸ì§€ ìƒì„±: {dummy_path}")

    # ëª¨ë¸ ë¡œë“œ
    vla, processor = load_openvla_model()

    if vla is None:
        print("\nâŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")
        return

    # ì˜ˆì¸¡
    result = predict_action_with_tokens(
        vla,
        processor,
        dummy_path,
        "pick up the blue block",
        unnorm_key="bridge_orig"
    )

    # Token ë¶„ì„
    print("\n" + "=" * 70)
    print("ğŸ“Š Token ë¶„ì„")
    print("=" * 70)

    token_ids = result["action_token_ids"]
    vocab_size = 32000

    print(f"\nToken IDs:")
    for i, tid in enumerate(token_ids):
        bin_idx = tid - (vocab_size - 256)
        norm_val = result["normalized_actions"][i]
        real_val = result["action"][i]
        print(f"  Dim {i}: token_id={tid:5d} â†’ bin={bin_idx:3d} â†’ norm={norm_val:+.3f} â†’ real={real_val:+.6f}")

    return result


# ============================================================
# Main
# ============================================================

if __name__ == "__main__":
    import sys

    print("=" * 70)
    print("ğŸ¤– ì‹¤ì œ OpenVLA ëª¨ë¸ë¡œ Action Tokens ì–»ê¸°")
    print("=" * 70)

    print("\nì„ íƒí•˜ì„¸ìš”:")
    print("  1. ë”ë¯¸ ì´ë¯¸ì§€ë¡œ í…ŒìŠ¤íŠ¸")
    print("  2. ëŒ€í™”í˜• ëª¨ë“œ")
    print("  3. ì¢…ë£Œ")

    try:
        choice = input("\nì„ íƒ (1/2/3): ").strip()

        if choice == "1":
            example_with_dummy_image()

        elif choice == "2":
            vla, processor = load_openvla_model()
            if vla is not None:
                interactive_demo(vla, processor)

        elif choice == "3":
            print("ì¢…ë£Œí•©ë‹ˆë‹¤.")
            sys.exit(0)

        else:
            print("âŒ ì˜ëª»ëœ ì„ íƒ")

    except KeyboardInterrupt:
        print("\n\nì¢…ë£Œí•©ë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì—ëŸ¬ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
